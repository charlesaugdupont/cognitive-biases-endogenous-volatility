{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f13fbb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from utils import *\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from experiment import unpack_and_dequantize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rcParams.update({\"font.size\" : 15,\n",
    "                     \"figure.dpi\" : 100, \n",
    "                     \"grid.alpha\" : 0.3, \n",
    "                     \"axes.grid\": False, \n",
    "                     \"axes.axisbelow\" : True,\n",
    "                     \"figure.figsize\":(8,6),\n",
    "                     \"mathtext.fontset\":\"cm\",\n",
    "                     \"xtick.labelsize\": 14,\n",
    "                     \"ytick.labelsize\": 14,\n",
    "                     \"axes.labelsize\": 16, \n",
    "                     \"legend.fontsize\": 13.5})\n",
    "plt.rc(\"text\", usetex=False)\n",
    "plt.rc(\"font\", family=\"serif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1cf25b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processing EUT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:37<00:00, 27.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing PT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:37<00:00, 27.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CPT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:37<00:00, 27.43it/s]\n"
     ]
    }
   ],
   "source": [
    "amplitude_files = {\n",
    "    \"eut\": \"../eut_dominant_frequencies_amplitudes.pickle\",\n",
    "    \"pt\": \"../pt_dominant_frequencies_amplitudes.pickle\",\n",
    "    \"cpt\": \"../cpt_dominant_frequencies_amplitudes.pickle\"\n",
    "}\n",
    "\n",
    "models = [\"eut\", \"pt\", \"cpt\"]\n",
    "data_storage = {m: [] for m in models}\n",
    "\n",
    "N = 200\n",
    "\n",
    "for model in models:\n",
    "    dir_path = os.path.join(\"..\", model) # Adjust path\n",
    "    f_names = [f for f in os.listdir(dir_path)]\n",
    "    \n",
    "    # Load amplitudes for this model if available\n",
    "    amps = []\n",
    "    if amplitude_files[model] and os.path.exists(amplitude_files[model]):\n",
    "        with open(amplitude_files[model], \"rb\") as f:\n",
    "            raw_amps = pickle.load(f) \n",
    "            amps = [np.mean(x[\"amplitudes\"]) for x in raw_amps]\n",
    "    else:\n",
    "        amps = [0.0] * len(f_names) # Default to 0 if no file (e.g. EUT)\n",
    "\n",
    "    # Load simulation states\n",
    "    print(f\"Processing {model.upper()}...\")\n",
    "    for idx, f_name in enumerate(tqdm(f_names)):\n",
    "        if idx >= len(amps): break\n",
    "        try:\n",
    "            with open(os.path.join(dir_path, f_name), \"rb\") as f:\n",
    "                res = pickle.load(f)\n",
    "            \n",
    "            # Get last step\n",
    "            w = unpack_and_dequantize(res[\"wealth\"][:,-1], N)\n",
    "            h = unpack_and_dequantize(res[\"health\"][:,-1], N)\n",
    "            \n",
    "            # Store tuple: (w_array, h_array, scalar_amplitude)\n",
    "            data_storage[model].append((w, h, amps[idx]))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error {f_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f30de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "num_bins = 40\n",
    "AMP_THRESHOLD = 5\n",
    "\n",
    "# 1. PRE-CALCULATE GLOBAL LIMITS\n",
    "global_min_prob = 1.0\n",
    "global_max_prob = 0.0\n",
    "plot_data_cache = { 'row1': [], 'row2': [] } \n",
    "\n",
    "for model in models:\n",
    "    all_items = data_storage[model]\n",
    "    total_points = sum(len(item[0]) for item in all_items)\n",
    "    if total_points == 0: total_points = 1\n",
    "    \n",
    "    current_min_quantum = 1.0 / total_points\n",
    "    if current_min_quantum < global_min_prob:\n",
    "        global_min_prob = current_min_quantum\n",
    "\n",
    "    # --- Process Row 1 ---\n",
    "    filt_items_1 = [item for item in all_items if item[2] < AMP_THRESHOLD]\n",
    "    if filt_items_1:\n",
    "        w = np.concatenate([item[0] for item in filt_items_1])\n",
    "        h = np.concatenate([item[1] for item in filt_items_1])\n",
    "        weights = np.ones_like(w) / total_points\n",
    "        H, _, _ = np.histogram2d(w, h, bins=num_bins, range=[[0, N], [0, N]], weights=weights)\n",
    "        if H.max() > global_max_prob: global_max_prob = H.max()\n",
    "        plot_data_cache['row1'].append((w, h, weights))\n",
    "    else:\n",
    "        plot_data_cache['row1'].append(None)\n",
    "\n",
    "    # --- Process Row 2 ---\n",
    "    filt_items_2 = [item for item in all_items if item[2] >= AMP_THRESHOLD]\n",
    "    if filt_items_2:\n",
    "        w = np.concatenate([item[0] for item in filt_items_2])\n",
    "        h = np.concatenate([item[1] for item in filt_items_2])\n",
    "        weights = np.ones_like(w) / total_points\n",
    "        H, _, _ = np.histogram2d(w, h, bins=num_bins, range=[[0, N], [0, N]], weights=weights)\n",
    "        if H.max() > global_max_prob: global_max_prob = H.max()\n",
    "        plot_data_cache['row2'].append((w, h, weights))\n",
    "    else:\n",
    "        plot_data_cache['row2'].append(None)\n",
    "\n",
    "if global_max_prob == 0: global_max_prob = 1.0\n",
    "\n",
    "# Define the Shared Norm\n",
    "shared_norm = mcolors.LogNorm(vmin=global_min_prob, vmax=global_max_prob)\n",
    "\n",
    "# 2. PLOTTING\n",
    "fig, axes = plt.subplots(2, 3, figsize=(17, 10), sharex=True, sharey=True)\n",
    "titles = [\"Expected Utility Theory\", \"Prospect Theory\", \"Cumulative Prospect Theory\"]\n",
    "\n",
    "plt.subplots_adjust(right=0.85, wspace=0.1, hspace=0.2)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    # --- Row 1 ---\n",
    "    ax = axes[0, i]\n",
    "    data = plot_data_cache['row1'][i]\n",
    "    if data is not None:\n",
    "        w, h, weights = data\n",
    "        hb = ax.hexbin(w, h, C=weights, reduce_C_function=np.sum,\n",
    "                       gridsize=num_bins, cmap='inferno', norm=shared_norm,\n",
    "                       extent=[1, N, 1, N], mincnt=1)\n",
    "    else:\n",
    "        ax.text(100, 100, \"No Point Attractors\", ha='center', va='center')\n",
    "    ax.set_title(f\"{titles[i]}\\n(Point Attractors)\", fontsize=16)\n",
    "\n",
    "    # --- Row 2 ---\n",
    "    ax = axes[1, i]\n",
    "    data = plot_data_cache['row2'][i]\n",
    "    if data is not None:\n",
    "        w, h, weights = data\n",
    "        hb = ax.hexbin(w, h, C=weights, reduce_C_function=np.sum,\n",
    "                       gridsize=num_bins, cmap='inferno', norm=shared_norm,\n",
    "                       extent=[1, N, 1, N], mincnt=1)\n",
    "    else:\n",
    "        ax.text(100, 100, \"No Simulations\", ha='center', va='center', fontsize=12)\n",
    "        ax.set_facecolor(\"#f0f0f067\")\n",
    "    ax.set_title(f\"{titles[i]}\\n(Limit Cycles)\", fontsize=16)\n",
    "    ax.set_xlabel(\"Wealth\")\n",
    "\n",
    "axes[0,0].set_ylabel(\"Health\")\n",
    "axes[1,0].set_ylabel(\"Health\")\n",
    "\n",
    "# --- SET TICKS EXPLICITLY ---\n",
    "ticks = [1, 50, 100, 150, 200]\n",
    "for ax in axes.flat:\n",
    "    ax.set_xticks(ticks)\n",
    "    ax.set_yticks(ticks)\n",
    "\n",
    "# 3. CREATE SHARED COLORBAR\n",
    "sm = plt.cm.ScalarMappable(cmap='inferno', norm=shared_norm)\n",
    "sm.set_array([]) \n",
    "cbar_ax = fig.add_axes([0.87, 0.15, 0.02, 0.7]) \n",
    "\n",
    "# --- UPDATED FORMATTER ---\n",
    "def decimal_formatter(x, pos):\n",
    "    # For very small probabilities, use 10^-x notation\n",
    "    if x < 0.001:\n",
    "        s = f'{x:.0e}' # e.g. \"1e-05\"\n",
    "        base, exponent = s.split('e')\n",
    "        # If the number is exactly 1*10^-x, return 10^-x\n",
    "        if float(base) == 1.0:\n",
    "            return r'$10^{{{}}}$'.format(int(exponent))\n",
    "        # Otherwise return standard scientific e.g. 2e-5\n",
    "        return s \n",
    "        \n",
    "    # For larger probabilities, keep decimal\n",
    "    if x < 0.01:  return f'{x:.3f}'\n",
    "    return f'{x:.2f}'\n",
    "\n",
    "cb = fig.colorbar(sm, cax=cbar_ax, format=ticker.FuncFormatter(decimal_formatter))\n",
    "cb.set_label('Probability', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051dfaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wealth-health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
